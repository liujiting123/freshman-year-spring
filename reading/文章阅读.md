# è¯´æ˜
è¯»è¿‡çš„æ–‡ç« è¿˜æ˜¯æ¯”æœ‰ç¬”è®°çš„è¦å¤šä¸€ç‚¹ï¼Œä½†æ˜¯å¤§å¤šæ•°æˆ‘éƒ½æ²¡æœ‰åƒè¿™æ ·åšç¬”è®°ï¼Œè¿™æ˜¯åé¢æ‰å¼€å‘å‡ºæ¥çš„ä¹ æƒ¯ã€‚
ä¸»è¦çš„æ–‡ç« ï¼ˆæˆ‘è®°å¾—åˆ°æˆ‘è¯»è¿‡çš„ä¸”æœ‰ç‚¹ç”¨çš„ï¼‰åŒ…æ‹¬ï¼š

---

## ğŸ§  é€šç”¨æ·±åº¦å­¦ä¹ ä¸ç”Ÿæˆæ¨¡å‹

- GAN: Generative Adversarial Nets â€” Ian Goodfellow et al., 2014  
- WGAN: Wasserstein GAN â€” Martin Arjovsky et al., 2017  
- Transformer: Attention Is All You Need â€” Vaswani et al., 2017  
- BERT: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding â€” Devlin et al., 2018  

---

## ğŸ–¼ï¸ å›¾åƒç”Ÿæˆä¸ç†è§£

- CLIP: Learning Transferable Visual Models From Natural Language Supervision â€” Alec Radford et al., 2021  
- DINOv2: DINOv2: Learning Robust Visual Features without Supervision â€” Meta AI, 2023  
- Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models â€” CompVis, 2022  
- Diffusion Models: Denoising Diffusion Probabilistic Models â€” Ho et al., 2020  

---

## ğŸŒŠ åˆ†å¸ƒå­¦ä¹ ä¸è¡¨ç¤ºå¯¹é½

- Flow Matching: Flow Matching for Generative Modeling â€” Lipman et al., 2023  

---

## ğŸ¦¾ å…·èº«æ™ºèƒ½ Embodied AI

- ACT: Action Chunking with Transformers â€” Yao et al., 2023  
- SayCan: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances â€” Google Research, 2022  
- Pi0: Policy Improvement for Language-Conditioned Robotics â€” Google DeepMind, 2024  
- GR00T: Grounded Robot Reasoning with Optimized and Opened Transformers â€” NVIDIA, 2024  
- OpenVLA: Generalist Foundation Model for Vision-Language-Action â€” Alibaba DAMO, 2024  
- One-2-VLA: One Foundation Model for Vision-Language-Action â€” ZJU & ByteDance, 2024  


å› ä¸ºå½“æ—¶æ‰¾æ–¹å‘çš„æ—¶å€™ï¼Œåƒäººè„¸è¯†åˆ«ã€çº¢å¤–ç”Ÿæˆã€ç¬¦å·å›å½’ä¹Ÿéƒ½çœ‹è¿‡ä¸€ç‚¹ï¼Œä¸è¿‡å¿˜å¾—å·®ä¸å¤šäº†ï¼Œå¹²è„†æ‡’å¾—ç¿»äº†ã€‚è¿˜æœ‰ä¸€äº›ç»¼è¿°æ–‡ç« ï¼Œè®°å½•ä¹Ÿåˆ äº†æˆ‘ä¹Ÿæ‰¾ä¸åˆ°ã€‚æ—æ—æ€»æ€»çš„åº”è¯¥è¿˜æ˜¯è¯»äº†ä¸€äº›
